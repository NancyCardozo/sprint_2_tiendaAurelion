"""
PROYECTO TIENDA AURELION - FASE 2
Estad√≠stica Aplicada: An√°lisis Descriptivo, Distribuciones, Correlaciones y Outliers
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import normaltest, shapiro, kstest, chi2_contingency
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURACI√ìN
# ============================================================================
CARPETA_LIMPIOS = 'datos_limpios'
CARPETA_ESTADISTICAS = 'estadisticas'

import os
os.makedirs(CARPETA_ESTADISTICAS, exist_ok=True)

print("="*80)
print("FASE 2: ESTAD√çSTICA APLICADA - TIENDA AURELION")
print("="*80)

# ============================================================================
# 1. LECTURA DE DATOS LIMPIOS
# ============================================================================
print("\n1. Cargando datos limpios...")

clientes = pd.read_csv(f'{CARPETA_LIMPIOS}/clientes_limpios.csv')
productos = pd.read_csv(f'{CARPETA_LIMPIOS}/productos_limpios.csv')
ventas = pd.read_csv(f'{CARPETA_LIMPIOS}/ventas_limpias.csv')
detalle_ventas = pd.read_csv(f'{CARPETA_LIMPIOS}/detalle_ventas_limpios.csv')
calendario = pd.read_csv(f'{CARPETA_LIMPIOS}/calendario.csv')

# Convertir fechas
ventas['fecha'] = pd.to_datetime(ventas['fecha'])
calendario['fecha'] = pd.to_datetime(calendario['fecha'])
clientes['fecha_alta'] = pd.to_datetime(clientes['fecha_alta'])

print(f"   ‚úì Datos cargados exitosamente")

# ============================================================================
# 2. CONSTRUCCI√ìN DE DATASET CONSOLIDADO PARA AN√ÅLISIS
# ============================================================================
print("\n2. Construyendo dataset consolidado...")

# Unir detalle_ventas con ventas
ventas_completas = detalle_ventas.merge(ventas, on='id_venta', how='left')

# Agregar informaci√≥n de productos (sin precio_unitario para evitar duplicados)
productos_sin_precio = productos.drop('precio_unitario', axis=1)
ventas_completas = ventas_completas.merge(productos_sin_precio, on='id_producto', how='left')

# Agregar informaci√≥n de clientes
ventas_completas = ventas_completas.merge(
    clientes[['id_cliente', 'ciudad']], 
    on='id_cliente', 
    how='left'
)

# Agregar informaci√≥n temporal
ventas_completas = ventas_completas.merge(
    calendario[['fecha', 'anio', 'mes', 'dia_semana', 'nombre_dia']], 
    on='fecha', 
    how='left'
)

print(f"   ‚úì Dataset consolidado: {ventas_completas.shape[0]} registros")
print(f"   ‚úì Columnas disponibles: {list(ventas_completas.columns)}")

# ============================================================================
# 3. ESTAD√çSTICAS DESCRIPTIVAS B√ÅSICAS
# ============================================================================
print("\n3. Calculando estad√≠sticas descriptivas b√°sicas...")

# 3.1 Variables Num√©ricas Principales
# Verificar nombres exactos de columnas
print(f"\n   DEBUG: Columnas num√©ricas encontradas:")
for col in ventas_completas.select_dtypes(include=[np.number]).columns:
    print(f"      - {col}")

variables_numericas = {
    'cantidad': ventas_completas['cantidad'],
    'precio_unitario': ventas_completas['precio_unitario'],  # Del detalle_ventas
    'importe': ventas_completas['importe'],
}

estadisticas_descriptivas = []

for nombre_var, variable in variables_numericas.items():
    stats_dict = {
        'Variable': nombre_var,
        'Count': len(variable),
        'Mean': variable.mean(),
        'Median': variable.median(),
        'Mode': variable.mode()[0] if len(variable.mode()) > 0 else np.nan,
        'Std': variable.std(),
        'Variance': variable.var(),
        'Min': variable.min(),
        'Q1': variable.quantile(0.25),
        'Q2': variable.quantile(0.50),
        'Q3': variable.quantile(0.75),
        'Max': variable.max(),
        'Range': variable.max() - variable.min(),
        'IQR': variable.quantile(0.75) - variable.quantile(0.25),
        'CV': (variable.std() / variable.mean()) * 100,  # Coeficiente de variaci√≥n
        'Skewness': variable.skew(),
        'Kurtosis': variable.kurtosis()
    }
    estadisticas_descriptivas.append(stats_dict)

df_estadisticas = pd.DataFrame(estadisticas_descriptivas)

# 3.2 Estad√≠sticas por Categor√≠a
print("\n   Calculando estad√≠sticas por categor√≠a...")

stats_por_categoria = ventas_completas.groupby('categoria').agg({
    'importe': ['count', 'sum', 'mean', 'median', 'std'],
    'cantidad': ['sum', 'mean'],
    'id_venta': 'nunique'
}).round(2)

stats_por_categoria.columns = ['_'.join(col).strip() for col in stats_por_categoria.columns]
stats_por_categoria = stats_por_categoria.reset_index()

# 3.3 Estad√≠sticas por Ciudad
stats_por_ciudad = ventas_completas.groupby('ciudad').agg({
    'importe': ['count', 'sum', 'mean', 'median'],
    'id_venta': 'nunique',
    'id_cliente': 'nunique'
}).round(2)

stats_por_ciudad.columns = ['_'.join(col).strip() for col in stats_por_ciudad.columns]
stats_por_ciudad = stats_por_ciudad.reset_index()

# 3.4 Estad√≠sticas por Medio de Pago
stats_por_medio_pago = ventas_completas.groupby('medio_pago').agg({
    'importe': ['count', 'sum', 'mean', 'median', 'std'],
    'id_venta': 'nunique'
}).round(2)

stats_por_medio_pago.columns = ['_'.join(col).strip() for col in stats_por_medio_pago.columns]
stats_por_medio_pago = stats_por_medio_pago.reset_index()

# 3.5 Estad√≠sticas Temporales
stats_temporales = ventas_completas.groupby(['mes']).agg({
    'importe': ['count', 'sum', 'mean'],
    'id_venta': 'nunique',
    'cantidad': 'sum'
}).round(2)

stats_temporales.columns = ['_'.join(col).strip() for col in stats_temporales.columns]
stats_temporales = stats_temporales.reset_index()

print("   ‚úì Estad√≠sticas descriptivas calculadas")

# ============================================================================
# 4. IDENTIFICACI√ìN DE DISTRIBUCIONES
# ============================================================================
print("\n4. Identificando tipos de distribuci√≥n...")

distribuciones = []

for nombre_var, variable in variables_numericas.items():
    
    # Test de Normalidad: Shapiro-Wilk (para n < 5000)
    if len(variable) < 5000:
        shapiro_stat, shapiro_p = shapiro(variable)
    else:
        shapiro_stat, shapiro_p = np.nan, np.nan
    
    # Test de Normalidad: D'Agostino-Pearson
    dagostino_stat, dagostino_p = normaltest(variable)
    
    # Determinar tipo de distribuci√≥n
    if shapiro_p > 0.05 or dagostino_p > 0.05:
        tipo_dist = "Normal (aproximada)"
    else:
        # Analizar skewness y kurtosis
        skew = variable.skew()
        kurt = variable.kurtosis()
        
        if abs(skew) < 0.5 and abs(kurt) < 0.5:
            tipo_dist = "Sim√©trica (no normal)"
        elif skew > 1:
            tipo_dist = "Sesgada a la derecha (positiva)"
        elif skew < -1:
            tipo_dist = "Sesgada a la izquierda (negativa)"
        elif kurt > 1:
            tipo_dist = "Leptoc√∫rtica (picos altos)"
        elif kurt < -1:
            tipo_dist = "Platic√∫rtica (picos bajos)"
        else:
            tipo_dist = "No normal"
    
    distribuciones.append({
        'Variable': nombre_var,
        'Tipo_Distribucion': tipo_dist,
        'Shapiro_Wilk_Statistic': shapiro_stat,
        'Shapiro_Wilk_p_value': shapiro_p,
        'Shapiro_Normal': 'S√≠' if shapiro_p > 0.05 else 'No',
        'DAgostino_Statistic': dagostino_stat,
        'DAgostino_p_value': dagostino_p,
        'DAgostino_Normal': 'S√≠' if dagostino_p > 0.05 else 'No',
        'Skewness': variable.skew(),
        'Kurtosis': variable.kurtosis(),
        'Interpretacion_Skewness': 'Derecha' if variable.skew() > 0.5 else ('Izquierda' if variable.skew() < -0.5 else 'Sim√©trica'),
        'Interpretacion_Kurtosis': 'Leptoc√∫rtica' if variable.kurtosis() > 1 else ('Platic√∫rtica' if variable.kurtosis() < -1 else 'Mesoc√∫rtica')
    })

df_distribuciones = pd.DataFrame(distribuciones)

print("   ‚úì Distribuciones identificadas")

# ============================================================================
# 5. AN√ÅLISIS DE CORRELACIONES
# ============================================================================
print("\n5. Calculando correlaciones entre variables...")

# 5.1 Correlaci√≥n entre variables num√©ricas
variables_para_correlacion = ventas_completas[['cantidad', 'precio_unitario', 'importe', 'mes', 'dia_semana']]

# Matriz de correlaci√≥n de Pearson
correlacion_pearson = variables_para_correlacion.corr(method='pearson')

# Matriz de correlaci√≥n de Spearman (para datos no normales)
correlacion_spearman = variables_para_correlacion.corr(method='spearman')

# 5.2 Correlaciones espec√≠ficas de inter√©s
correlaciones_clave = []

pares_interes = [
    ('cantidad', 'importe'),
    ('precio_unitario', 'importe'),
    ('cantidad', 'precio_unitario'),
    ('mes', 'importe'),
    ('dia_semana', 'importe')
]

for var1, var2 in pares_interes:
    # Pearson
    pearson_r, pearson_p = stats.pearsonr(
        ventas_completas[var1].dropna(), 
        ventas_completas[var2].dropna()
    )
    
    # Spearman
    spearman_r, spearman_p = stats.spearmanr(
        ventas_completas[var1].dropna(), 
        ventas_completas[var2].dropna()
    )
    
    # Interpretaci√≥n
    if abs(pearson_r) > 0.7:
        fuerza = "Fuerte"
    elif abs(pearson_r) > 0.4:
        fuerza = "Moderada"
    elif abs(pearson_r) > 0.2:
        fuerza = "D√©bil"
    else:
        fuerza = "Muy d√©bil/Nula"
    
    direccion = "Positiva" if pearson_r > 0 else "Negativa"
    
    correlaciones_clave.append({
        'Variable_1': var1,
        'Variable_2': var2,
        'Pearson_r': pearson_r,
        'Pearson_p_value': pearson_p,
        'Pearson_Significativo': 'S√≠' if pearson_p < 0.05 else 'No',
        'Spearman_r': spearman_r,
        'Spearman_p_value': spearman_p,
        'Spearman_Significativo': 'S√≠' if spearman_p < 0.05 else 'No',
        'Fuerza': fuerza,
        'Direccion': direccion,
        'Interpretacion': f"{fuerza} {direccion}"
    })

df_correlaciones = pd.DataFrame(correlaciones_clave)

# 5.3 Correlaci√≥n con variables categ√≥ricas (Chi-cuadrado)
print("\n   Calculando correlaciones con variables categ√≥ricas...")

# Chi-cuadrado: Categor√≠a vs Medio de Pago
tabla_contingencia = pd.crosstab(ventas_completas['categoria'], ventas_completas['medio_pago'])
chi2, p_value, dof, expected = chi2_contingency(tabla_contingencia)

correlacion_categoricas = {
    'Test': 'Chi-cuadrado',
    'Variable_1': 'categoria',
    'Variable_2': 'medio_pago',
    'Chi2_Statistic': chi2,
    'p_value': p_value,
    'Grados_Libertad': dof,
    'Asociacion_Significativa': 'S√≠' if p_value < 0.05 else 'No'
}

print("   ‚úì Correlaciones calculadas")

# ============================================================================
# 6. DETECCI√ìN DE OUTLIERS (IQR)
# ============================================================================
print("\n6. Detectando outliers mediante IQR...")

outliers_resultados = []

for nombre_var, variable in variables_numericas.items():
    Q1 = variable.quantile(0.25)
    Q3 = variable.quantile(0.75)
    IQR = Q3 - Q1
    
    # L√≠mites
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Outliers extremos
    lower_extreme = Q1 - 3 * IQR
    upper_extreme = Q3 + 3 * IQR
    
    # Detectar outliers
    outliers_leves = variable[(variable < lower_bound) | (variable > upper_bound)]
    outliers_extremos = variable[(variable < lower_extreme) | (variable > upper_extreme)]
    
    # Porcentajes
    pct_outliers_leves = (len(outliers_leves) / len(variable)) * 100
    pct_outliers_extremos = (len(outliers_extremos) / len(variable)) * 100
    
    outliers_resultados.append({
        'Variable': nombre_var,
        'Q1': Q1,
        'Q3': Q3,
        'IQR': IQR,
        'Limite_Inferior': lower_bound,
        'Limite_Superior': upper_bound,
        'Limite_Inferior_Extremo': lower_extreme,
        'Limite_Superior_Extremo': upper_extreme,
        'Outliers_Leves_Count': len(outliers_leves),
        'Outliers_Leves_Pct': pct_outliers_leves,
        'Outliers_Extremos_Count': len(outliers_extremos),
        'Outliers_Extremos_Pct': pct_outliers_extremos,
        'Min_Outlier': outliers_leves.min() if len(outliers_leves) > 0 else np.nan,
        'Max_Outlier': outliers_leves.max() if len(outliers_leves) > 0 else np.nan,
        'Interpretacion': 'Sin outliers significativos' if pct_outliers_leves < 5 else 'Outliers presentes'
    })

df_outliers = pd.DataFrame(outliers_resultados)

# 6.2 An√°lisis espec√≠fico de outliers por variable
print("\n   Analizando outliers en detalle...")

# Outliers en IMPORTE
outliers_importe = ventas_completas[
    (ventas_completas['importe'] > df_outliers[df_outliers['Variable'] == 'importe']['Limite_Superior'].values[0])
]

top_outliers_importe = outliers_importe.nlargest(10, 'importe')[
    ['id_venta', 'nombre_producto', 'cantidad', 'precio_unitario', 'importe', 'categoria']
]

print("   ‚úì Outliers detectados")

# ============================================================================
# 7. INTERPRETACI√ìN PARA EL NEGOCIO
# ============================================================================
print("\n7. Generando interpretaciones para el negocio...")

interpretaciones = []

# Interpretaci√≥n 1: Estad√≠sticas Descriptivas
for _, row in df_estadisticas.iterrows():
    if row['Variable'] == 'importe':
        interpretaciones.append({
            'Tipo_Analisis': 'Estad√≠stica Descriptiva',
            'Variable': 'Importe',
            'Hallazgo': f"Venta promedio: ${row['Mean']:,.0f} | Mediana: ${row['Median']:,.0f}",
            'Interpretacion': f"El ticket promedio (${row['Mean']:,.0f}) es {'mayor' if row['Mean'] > row['Median'] else 'menor'} que la mediana (${row['Median']:,.0f}), indicando {'ventas grandes que elevan el promedio' if row['Mean'] > row['Median'] else 'distribuci√≥n equilibrada'}.",
            'Impacto_Negocio': 'Alto',
            'Accion_Recomendada': 'Estrategias de upselling para aumentar ticket promedio' if row['Mean'] < 30000 else 'Mantener estrategia actual de venta'
        })

# Interpretaci√≥n 2: Distribuciones
for _, row in df_distribuciones.iterrows():
    if row['Variable'] == 'cantidad':
        interpretaciones.append({
            'Tipo_Analisis': 'Distribuci√≥n',
            'Variable': 'Cantidad',
            'Hallazgo': f"Distribuci√≥n: {row['Tipo_Distribucion']} | Skewness: {row['Skewness']:.2f}",
            'Interpretacion': f"La cantidad por venta tiene sesgo {row['Interpretacion_Skewness']}, lo que indica que {'la mayor√≠a compra pocas unidades con algunos clientes comprando mucho' if row['Skewness'] > 0.5 else 'las compras son equilibradas'}.",
            'Impacto_Negocio': 'Medio',
            'Accion_Recomendada': 'Promociones por volumen (3x2, descuentos por cantidad)' if row['Skewness'] > 0.5 else 'Mantener estrategia actual'
        })

# Interpretaci√≥n 3: Correlaciones
for _, row in df_correlaciones.iterrows():
    if row['Variable_1'] == 'cantidad' and row['Variable_2'] == 'importe':
        interpretaciones.append({
            'Tipo_Analisis': 'Correlaci√≥n',
            'Variable': 'Cantidad-Importe',
            'Hallazgo': f"Correlaci√≥n {row['Fuerza']} {row['Direccion']} (r={row['Pearson_r']:.3f})",
            'Interpretacion': f"{'Fuerte' if abs(row['Pearson_r']) > 0.7 else 'Moderada'} relaci√≥n entre cantidad e importe, como se espera. {'Clientes que compran m√°s unidades gastan proporcionalmente m√°s.' if row['Pearson_r'] > 0 else 'Relaci√≥n inesperada.'}",
            'Impacto_Negocio': 'Bajo',
            'Accion_Recomendada': 'Esperado. Seguir monitoreando.'
        })
    
    if row['Variable_1'] == 'precio_unitario' and row['Variable_2'] == 'importe':
        interpretaciones.append({
            'Tipo_Analisis': 'Correlaci√≥n',
            'Variable': 'Precio-Importe',
            'Hallazgo': f"Correlaci√≥n {row['Fuerza']} {row['Direccion']} (r={row['Pearson_r']:.3f})",
            'Interpretacion': f"El precio unitario tiene {'fuerte' if abs(row['Pearson_r']) > 0.7 else 'moderada'} influencia en el importe final. Productos de mayor precio generan {'proporcionalmente' if row['Pearson_r'] > 0.8 else 'moderadamente'} m√°s ingresos.",
            'Impacto_Negocio': 'Alto',
            'Accion_Recomendada': 'Promover productos de alto valor (Ron, Yerba, Desodorante)' if abs(row['Pearson_r']) > 0.5 else 'Revisar estrategia de pricing'
        })

# Interpretaci√≥n 4: Outliers
for _, row in df_outliers.iterrows():
    if row['Variable'] == 'importe' and row['Outliers_Leves_Pct'] > 5:
        interpretaciones.append({
            'Tipo_Analisis': 'Outliers',
            'Variable': 'Importe',
            'Hallazgo': f"{row['Outliers_Leves_Count']} outliers ({row['Outliers_Leves_Pct']:.1f}%)",
            'Interpretacion': f"Existen {row['Outliers_Leves_Count']} ventas con importes inusualmente altos (>${row['Limite_Superior']:,.0f}). Estas representan el {row['Outliers_Leves_Pct']:.1f}% de las transacciones.",
            'Impacto_Negocio': 'Alto',
            'Accion_Recomendada': 'Analizar estas ventas grandes: ¬øSon clientes VIP? ¬øCompras corporativas? Replicar comportamiento.'
        })

df_interpretaciones = pd.DataFrame(interpretaciones)

print("   ‚úì Interpretaciones generadas")

# ============================================================================
# 8. TABLA DE HERRAMIENTAS Y M√âTODOS UTILIZADOS
# ============================================================================
print("\n8. Documentando herramientas y m√©todos...")

herramientas_metodos = [
    # ESTAD√çSTICAS DESCRIPTIVAS
    {
        'Categoria': 'Estad√≠stica Descriptiva',
        'Herramienta': 'Medidas de Tendencia Central',
        'Metodo_Python': 'mean(), median(), mode()',
        'Libreria': 'pandas, numpy',
        'Aplicacion': 'Calcular promedio, mediana y moda de cantidad, precio e importe',
        'Resultado': 'Identificar valores t√≠picos de ventas',
        'Ejemplo_Codigo': 'df["importe"].mean()'
    },
    {
        'Categoria': 'Estad√≠stica Descriptiva',
        'Herramienta': 'Medidas de Dispersi√≥n',
        'Metodo_Python': 'std(), var(), quantile()',
        'Libreria': 'pandas, numpy',
        'Aplicacion': 'Medir variabilidad de ventas',
        'Resultado': 'Desviaci√≥n est√°ndar, varianza, cuartiles',
        'Ejemplo_Codigo': 'df["importe"].std()'
    },
    {
        'Categoria': 'Estad√≠stica Descriptiva',
        'Herramienta': 'Medidas de Forma',
        'Metodo_Python': 'skew(), kurtosis()',
        'Libreria': 'pandas, scipy.stats',
        'Aplicacion': 'Analizar simetr√≠a y curtosis de distribuciones',
        'Resultado': 'Identificar sesgos y forma de la distribuci√≥n',
        'Ejemplo_Codigo': 'df["cantidad"].skew()'
    },
    {
        'Categoria': 'Estad√≠stica Descriptiva',
        'Herramienta': 'Coeficiente de Variaci√≥n',
        'Metodo_Python': '(std() / mean()) * 100',
        'Libreria': 'pandas, numpy',
        'Aplicacion': 'Comparar variabilidad relativa entre variables',
        'Resultado': 'Porcentaje de variaci√≥n respecto a la media',
        'Ejemplo_Codigo': '(df["importe"].std() / df["importe"].mean()) * 100'
    },
    
    # DISTRIBUCIONES
    {
        'Categoria': 'An√°lisis de Distribuci√≥n',
        'Herramienta': 'Test de Shapiro-Wilk',
        'Metodo_Python': 'shapiro()',
        'Libreria': 'scipy.stats',
        'Aplicacion': 'Evaluar normalidad de variables (n < 5000)',
        'Resultado': 'p-value > 0.05 indica distribuci√≥n normal',
        'Ejemplo_Codigo': 'stats.shapiro(df["importe"])'
    },
    {
        'Categoria': 'An√°lisis de Distribuci√≥n',
        'Herramienta': 'Test de D\'Agostino-Pearson',
        'Metodo_Python': 'normaltest()',
        'Libreria': 'scipy.stats',
        'Aplicacion': 'Evaluar normalidad mediante skewness y kurtosis',
        'Resultado': 'Determinar si sigue distribuci√≥n normal',
        'Ejemplo_Codigo': 'stats.normaltest(df["cantidad"])'
    },
    {
        'Categoria': 'An√°lisis de Distribuci√≥n',
        'Herramienta': 'An√°lisis de Skewness',
        'Metodo_Python': 'skew()',
        'Libreria': 'pandas',
        'Aplicacion': 'Identificar sesgo de la distribuci√≥n',
        'Resultado': '>0: derecha, <0: izquierda, ‚âà0: sim√©trica',
        'Ejemplo_Codigo': 'df["precio_unitario"].skew()'
    },
    {
        'Categoria': 'An√°lisis de Distribuci√≥n',
        'Herramienta': 'An√°lisis de Kurtosis',
        'Metodo_Python': 'kurtosis()',
        'Libreria': 'pandas',
        'Aplicacion': 'Identificar forma de picos en distribuci√≥n',
        'Resultado': '>0: leptoc√∫rtica, <0: platic√∫rtica',
        'Ejemplo_Codigo': 'df["importe"].kurtosis()'
    },
    
    # CORRELACIONES
    {
        'Categoria': 'An√°lisis de Correlaci√≥n',
        'Herramienta': 'Correlaci√≥n de Pearson',
        'Metodo_Python': 'corr(method="pearson"), pearsonr()',
        'Libreria': 'pandas, scipy.stats',
        'Aplicacion': 'Medir relaci√≥n lineal entre variables num√©ricas',
        'Resultado': 'r entre -1 y 1 (fuerza y direcci√≥n)',
        'Ejemplo_Codigo': 'df[["cantidad", "importe"]].corr()'
    },
    {
        'Categoria': 'An√°lisis de Correlaci√≥n',
        'Herramienta': 'Correlaci√≥n de Spearman',
        'Metodo_Python': 'corr(method="spearman"), spearmanr()',
        'Libreria': 'pandas, scipy.stats',
        'Aplicacion': 'Medir relaci√≥n monot√≥nica (datos no normales)',
        'Resultado': 'Correlaci√≥n basada en rangos',
        'Ejemplo_Codigo': 'stats.spearmanr(df["cantidad"], df["importe"])'
    },
    {
        'Categoria': 'An√°lisis de Correlaci√≥n',
        'Herramienta': 'Matriz de Correlaci√≥n',
        'Metodo_Python': 'corr()',
        'Libreria': 'pandas',
        'Aplicacion': 'Crear matriz de correlaciones m√∫ltiples',
        'Resultado': 'Tabla con todas las correlaciones',
        'Ejemplo_Codigo': 'df[cols_numericas].corr()'
    },
    {
        'Categoria': 'An√°lisis de Correlaci√≥n',
        'Herramienta': 'Test Chi-cuadrado',
        'Metodo_Python': 'chi2_contingency()',
        'Libreria': 'scipy.stats',
        'Aplicacion': 'Evaluar asociaci√≥n entre variables categ√≥ricas',
        'Resultado': 'p-value < 0.05 indica asociaci√≥n significativa',
        'Ejemplo_Codigo': 'stats.chi2_contingency(pd.crosstab(df["cat1"], df["cat2"]))'
    },
    
    # OUTLIERS
    {
        'Categoria': 'Detecci√≥n de Outliers',
        'Herramienta': 'M√©todo IQR (Rango Intercuart√≠lico)',
        'Metodo_Python': 'quantile(0.25), quantile(0.75)',
        'Libreria': 'pandas, numpy',
        'Aplicacion': 'Identificar valores at√≠picos',
        'Resultado': 'Outliers: Q1-1.5*IQR o Q3+1.5*IQR',
        'Ejemplo_Codigo': 'Q1 = df["importe"].quantile(0.25); IQR = Q3 - Q1'
    },
    {
        'Categoria': 'Detecci√≥n de Outliers',
        'Herramienta': 'L√≠mites de Outliers Extremos',
        'Metodo_Python': 'Q1 - 3*IQR, Q3 + 3*IQR',
        'Libreria': 'pandas, numpy',
        'Aplicacion': 'Detectar valores extremadamente at√≠picos',
        'Resultado': 'Outliers m√°s all√° de 3*IQR',
        'Ejemplo_Codigo': 'upper_extreme = Q3 + 3 * IQR'
    },
    {
        'Categoria': 'Detecci√≥n de Outliers',
        'Herramienta': 'Filtrado Booleano',
        'Metodo_Python': 'Boolean indexing',
        'Libreria': 'pandas',
        'Aplicacion': 'Extraer registros que son outliers',
        'Resultado': 'DataFrame con solo outliers',
        'Ejemplo_Codigo': 'outliers = df[df["importe"] > upper_bound]'
    },
    {
        'Categoria': 'Detecci√≥n de Outliers',
        'Herramienta': 'Porcentaje de Outliers',
        'Metodo_Python': 'len() / len() * 100',
        'Libreria': 'pandas, numpy',
        'Aplicacion': 'Calcular proporci√≥n de outliers',
        'Resultado': 'Porcentaje de valores at√≠picos',
        'Ejemplo_Codigo': '(len(outliers) / len(df)) * 100'
    },
    
    # AGRUPACIONES
    {
        'Categoria': 'An√°lisis por Grupos',
        'Herramienta': 'GroupBy con Agregaciones',
        'Metodo_Python': 'groupby().agg()',
        'Libreria': 'pandas',
        'Aplicacion': 'Calcular estad√≠sticas por categor√≠a/ciudad/etc',
        'Resultado': 'Estad√≠sticas segmentadas',
        'Ejemplo_Codigo': 'df.groupby("categoria").agg({"importe": ["mean", "sum"]})'
    },
    {
        'Categoria': 'An√°lisis por Grupos',
        'Herramienta': 'Tablas de Contingencia',
        'Metodo_Python': 'pd.crosstab()',
        'Libreria': 'pandas',
        'Aplicacion': 'Crear tablas de frecuencia cruzada',
        'Resultado': 'Matriz de conteos por categor√≠as',
        'Ejemplo_Codigo': 'pd.crosstab(df["categoria"], df["medio_pago"])'
    },
    
    # MANIPULACI√ìN DE DATOS
    {
        'Categoria': 'Preparaci√≥n de Datos',
        'Herramienta': 'Merge/Join',
        'Metodo_Python': 'merge()',
        'Libreria': 'pandas',
        'Aplicacion': 'Unir m√∫ltiples DataFrames',
        'Resultado': 'Dataset consolidado para an√°lisis',
        'Ejemplo_Codigo': 'df1.merge(df2, on="id", how="left")'
    },
    {
        'Categoria': 'Preparaci√≥n de Datos',
        'Herramienta': 'Conversi√≥n de Tipos',
        'Metodo_Python': 'pd.to_datetime(), astype()',
        'Libreria': 'pandas',
        'Aplicacion': 'Convertir tipos de datos',
        'Resultado': 'Datos en formato correcto para an√°lisis',
        'Ejemplo_Codigo': 'df["fecha"] = pd.to_datetime(df["fecha"])'
    },
    {
        'Categoria': 'Preparaci√≥n de Datos',
        'Herramienta': 'Manejo de Valores Nulos',
        'Metodo_Python': 'dropna(), isna()',
        'Libreria': 'pandas',
        'Aplicacion': 'Eliminar o identificar valores nulos',
        'Resultado': 'Dataset limpio para c√°lculos',
        'Ejemplo_Codigo': 'df["col"].dropna()'
    }
]

df_herramientas = pd.DataFrame(herramientas_metodos)

print("   ‚úì Tabla de herramientas documentada")

# ============================================================================
# 9. GUARDAR RESULTADOS
# ============================================================================
print("\n9. Guardando resultados del an√°lisis estad√≠stico...")

# Guardar todas las tablas
df_estadisticas.to_csv(f'{CARPETA_ESTADISTICAS}/01_estadisticas_descriptivas.csv', index=False)
df_distribuciones.to_csv(f'{CARPETA_ESTADISTICAS}/02_analisis_distribuciones.csv', index=False)
df_correlaciones.to_csv(f'{CARPETA_ESTADISTICAS}/03_correlaciones.csv', index=False)
correlacion_pearson.to_csv(f'{CARPETA_ESTADISTICAS}/04_matriz_correlacion_pearson.csv')
correlacion_spearman.to_csv(f'{CARPETA_ESTADISTICAS}/05_matriz_correlacion_spearman.csv')
df_outliers.to_csv(f'{CARPETA_ESTADISTICAS}/06_analisis_outliers.csv', index=False)
top_outliers_importe.to_csv(f'{CARPETA_ESTADISTICAS}/07_top_outliers_importe.csv', index=False)
df_interpretaciones.to_csv(f'{CARPETA_ESTADISTICAS}/08_interpretaciones_negocio.csv', index=False)
df_herramientas.to_csv(f'{CARPETA_ESTADISTICAS}/09_herramientas_metodos.csv', index=False)

# Guardar estad√≠sticas por segmento
stats_por_categoria.to_csv(f'{CARPETA_ESTADISTICAS}/10_stats_por_categoria.csv', index=False)
stats_por_ciudad.to_csv(f'{CARPETA_ESTADISTICAS}/11_stats_por_ciudad.csv', index=False)
stats_por_medio_pago.to_csv(f'{CARPETA_ESTADISTICAS}/12_stats_por_medio_pago.csv', index=False)
stats_temporales.to_csv(f'{CARPETA_ESTADISTICAS}/13_stats_temporales.csv', index=False)

print(f"   ‚úì 01_estadisticas_descriptivas.csv")
print(f"   ‚úì 02_analisis_distribuciones.csv")
print(f"   ‚úì 03_correlaciones.csv")
print(f"   ‚úì 04_matriz_correlacion_pearson.csv")
print(f"   ‚úì 05_matriz_correlacion_spearman.csv")
print(f"   ‚úì 06_analisis_outliers.csv")
print(f"   ‚úì 07_top_outliers_importe.csv")
print(f"   ‚úì 08_interpretaciones_negocio.csv")
print(f"   ‚úì 09_herramientas_metodos.csv")
print(f"   ‚úì 10_stats_por_categoria.csv")
print(f"   ‚úì 11_stats_por_ciudad.csv")
print(f"   ‚úì 12_stats_por_medio_pago.csv")
print(f"   ‚úì 13_stats_temporales.csv")

# ============================================================================
# 10. RESUMEN EJECUTIVO DEL AN√ÅLISIS
# ============================================================================
print("\n" + "="*80)
print("RESUMEN EJECUTIVO - FASE 2: ESTAD√çSTICA APLICADA")
print("="*80)

print("\nüìä ESTAD√çSTICAS DESCRIPTIVAS CLAVE:")
print("-" * 80)
for _, row in df_estadisticas.iterrows():
    print(f"\n{row['Variable'].upper()}:")
    print(f"  ‚Ä¢ Media: ${row['Mean']:,.2f}")
    print(f"  ‚Ä¢ Mediana: ${row['Median']:,.2f}")
    print(f"  ‚Ä¢ Desviaci√≥n Std: ${row['Std']:,.2f}")
    print(f"  ‚Ä¢ Rango: ${row['Min']:,.2f} - ${row['Max']:,.2f}")
    print(f"  ‚Ä¢ Coeficiente Variaci√≥n: {row['CV']:.2f}%")
    print(f"  ‚Ä¢ Skewness: {row['Skewness']:.3f} ({'Derecha' if row['Skewness'] > 0 else 'Izquierda'})")

print("\nüìà DISTRIBUCIONES IDENTIFICADAS:")
print("-" * 80)
for _, row in df_distribuciones.iterrows():
    print(f"\n{row['Variable'].upper()}: {row['Tipo_Distribucion']}")
    print(f"  ‚Ä¢ Test Shapiro-Wilk: {'Normal' if row['Shapiro_Normal'] == 'S√≠' else 'No Normal'} (p={row['Shapiro_Wilk_p_value']:.4f})")
    print(f"  ‚Ä¢ Test D'Agostino: {'Normal' if row['DAgostino_Normal'] == 'S√≠' else 'No Normal'} (p={row['DAgostino_p_value']:.4f})")
    print(f"  ‚Ä¢ Interpretaci√≥n: {row['Interpretacion_Skewness']} con curtosis {row['Interpretacion_Kurtosis']}")

print("\nüîó CORRELACIONES PRINCIPALES:")
print("-" * 80)
for _, row in df_correlaciones.iterrows():
    print(f"\n{row['Variable_1'].upper()} ‚Üî {row['Variable_2'].upper()}:")
    print(f"  ‚Ä¢ Pearson r: {row['Pearson_r']:.3f} ({row['Fuerza']} {row['Direccion']})")
    print(f"  ‚Ä¢ Spearman r: {row['Spearman_r']:.3f}")
    print(f"  ‚Ä¢ Significativo: {row['Pearson_Significativo']} (p={row['Pearson_p_value']:.4f})")
    print(f"  ‚Ä¢ Interpretaci√≥n: {row['Interpretacion']}")

print("\n‚ö†Ô∏è  OUTLIERS DETECTADOS:")
print("-" * 80)
for _, row in df_outliers.iterrows():
    print(f"\n{row['Variable'].upper()}:")
    print(f"  ‚Ä¢ Outliers leves: {row['Outliers_Leves_Count']} ({row['Outliers_Leves_Pct']:.2f}%)")
    print(f"  ‚Ä¢ Outliers extremos: {row['Outliers_Extremos_Count']} ({row['Outliers_Extremos_Pct']:.2f}%)")
    print(f"  ‚Ä¢ L√≠mite superior: ${row['Limite_Superior']:,.2f}")
    print(f"  ‚Ä¢ L√≠mite inferior: ${row['Limite_Inferior']:,.2f}")
    print(f"  ‚Ä¢ Estado: {row['Interpretacion']}")

print("\nüíº INTERPRETACIONES PARA EL NEGOCIO:")
print("-" * 80)
for i, row in df_interpretaciones.iterrows():
    print(f"\n{i+1}. {row['Tipo_Analisis']} - {row['Variable']}")
    print(f"   Hallazgo: {row['Hallazgo']}")
    print(f"   Interpretaci√≥n: {row['Interpretacion']}")
    print(f"   Impacto: {row['Impacto_Negocio']}")
    print(f"   Acci√≥n: {row['Accion_Recomendada']}")

print("\nüìä ESTAD√çSTICAS POR CATEGOR√çA:")
print("-" * 80)
print(stats_por_categoria.to_string(index=False))

print("\nüåç ESTAD√çSTICAS POR CIUDAD:")
print("-" * 80)
print(stats_por_ciudad.to_string(index=False))

print("\nüí≥ ESTAD√çSTICAS POR MEDIO DE PAGO:")
print("-" * 80)
print(stats_por_medio_pago.to_string(index=False))

print("\nüìÖ ESTAD√çSTICAS TEMPORALES:")
print("-" * 80)
print(stats_temporales.to_string(index=False))

print("\n" + "="*80)
print("‚úì FASE 2 COMPLETADA EXITOSAMENTE")
print(f"‚úì {13} archivos generados en '{CARPETA_ESTADISTICAS}/'")
print("="*80)

# ============================================================================
# 11. AN√ÅLISIS ADICIONAL: INSIGHTS CLAVE
# ============================================================================
print("\n" + "="*80)
print("INSIGHTS CLAVE PARA LA TOMA DE DECISIONES")
print("="*80)

# Insight 1: Productos m√°s rentables
print("\n1. PRODUCTOS M√ÅS RENTABLES:")
top_productos = ventas_completas.groupby('nombre_producto').agg({
    'importe': 'sum',
    'cantidad': 'sum'
}).sort_values('importe', ascending=False).head(5)
print(top_productos)

# Insight 2: Categor√≠a dominante
print("\n2. VENTAS POR CATEGOR√çA:")
ventas_categoria = ventas_completas.groupby('categoria')['importe'].sum()
print(f"   Alimentos: ${ventas_categoria.get('Alimentos', 0):,.0f} ({ventas_categoria.get('Alimentos', 0)/ventas_categoria.sum()*100:.1f}%)")
print(f"   Limpieza: ${ventas_categoria.get('Limpieza', 0):,.0f} ({ventas_categoria.get('Limpieza', 0)/ventas_categoria.sum()*100:.1f}%)")

# Insight 3: Mejor d√≠a de la semana
print("\n3. MEJOR D√çA DE LA SEMANA:")
ventas_por_dia = ventas_completas.groupby('nombre_dia')['importe'].sum().sort_values(ascending=False)
print(ventas_por_dia.head(3))

# Insight 4: Mes con m√°s ventas
print("\n4. MES CON M√ÅS VENTAS:")
meses = {1: 'Enero', 2: 'Febrero', 3: 'Marzo', 4: 'Abril', 5: 'Mayo', 6: 'Junio'}
ventas_por_mes = ventas_completas.groupby('mes')['importe'].sum().sort_values(ascending=False)
for mes, importe in ventas_por_mes.head(3).items():
    print(f"   {meses.get(mes, mes)}: ${importe:,.0f}")

# Insight 5: Ciudad m√°s rentable
print("\n5. CIUDAD M√ÅS RENTABLE:")
ventas_por_ciudad = ventas_completas.groupby('ciudad')['importe'].sum().sort_values(ascending=False)
print(ventas_por_ciudad.head(3))

print("\n" + "="*80)
print("FIN DEL AN√ÅLISIS ESTAD√çSTICO")
print("="*80)